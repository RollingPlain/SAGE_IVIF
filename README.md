# SAGE 🔥

<div align="center">

[![Paper](https://img.shields.io/badge/Paper-CVPR2025-green)](https://scholar.google.com.hk/scholar?hl=zh-CN&as_sdt=0%2C5&q=Every+SAM+Drop+Counts%3A+Embracing+Semantic+Priors+for+Multi-Modality+Image+Fusion+and+Beyond&btnG=)
[![GitHub Stars](https://img.shields.io/github/stars/RollingPlain/SAGE_IVIF?style=social)](https://github.com/RollingPlain/SAGE_IVIF)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.12+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

Official Code for: **"Every SAM Drop Counts: Embracing Semantic Priors for Multi-Modality Image Fusion and Beyond"**  
*Guanyao Wu, Haoyu Liu, Hongming Fu, Yichuan Peng, Jinyuan Liu, Xin Fan, Risheng Liu*  
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025
- [*[Google Scholar]*](https://scholar.google.com.hk/scholar?hl=zh-CN&as_sdt=0%2C5&q=Every+SAM+Drop+Counts%3A+Embracing+Semantic+Priors+for+Multi-Modality+Image+Fusion+and+Beyond&btnG=)

## 📝 Citation

If our work has been helpful to you, please feel free to cite our paper:

```bibtex
@inproceedings{wu2025sage,
  title={Every SAM Drop Counts: Embracing Semantic Priors for Multi-Modality Image Fusion and Beyond},
  author={Wu, Guanyao and Liu, Haoyu and Fu, Hongming and Peng, Yichuan and Liu, Jinyuan and Fan, Xin and Liu, Risheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}
```

## 🌟 Preview of SAGE

<div align="center">
  <img src="assets/fig.pdf" alt="Comparative Approaches" width="100%">
</div>

*__Figure 1:__ Differences between the proposed method and existing mainstream comparative approaches: **(a)** Traditional and early DL-based methods focus on the fusion visual effect. **(b)** Task-specific methods (e.g., TarDAL & SegMiF) introduce task loss and features that lead to inconsistent optimization goals, causing a conflict between visual and task accuracy. **(c)** Our pipeline first leverages semantic priors from SAM within a large network and then distills the knowledge into a smaller sub-network achieving practical inference feasibility while ensuring "the best of both worlds" through SAM's inherent adaptability to these tasks.*

<div align="center">
  <img src="assets/workflow1.jpg" alt="SAGE Workflow" width="100%">
</div>

*__Figure 2:__ Overall workflow of our proposed method. **(a)** The flow structure of the main network, where the SPA module processes patches with semantic priors generated by SAM. **(b)** The detailed structure of the SPA module, where PR plays a key role in preserving the source and integrating semantic information. **(c)** Our distillation scheme formulation, with visualizations of the different components of the triplet loss. **(d)** A simple diagram of the sub-network, composed of stacked dense blocks.*





## 🚀  Set Up on Your Own Machine

### Virtual Environment

We strongly recommend that you use Conda as a package manager.

```shell
# create virtual environment
conda create -n sage python=3.10
conda activate sage
# select and install pytorch version yourself (Necessary & Important)
# install requirements package
pip install -r requirements.txt
```
#### Data Preparation

You should put the data in the correct place in the following form.

```
SAGE ROOT
├── data
|   ├── test
|   |   ├── M3FD
|   |   |   ├── Ir # infrared images
|   |   |   └── Vis # visible images
|   |   ├── TNO
|   |   |   ├── Ir # infrared images
|   |   |   └── Vis # visible images
|   |   ├── RoadScene
|   |   └── ...
|   ├── train
|   |   ├── Ir # infrared images
|   |   ├── Vis # visible images
|   |   ├── Label # segmentation ground truth masks
|   |   └── Mask_cache # cached segmentation masks generated by SAM
```
### Test
This code natively supports the same naming for infrared and visible image pairs. An naming example can be found in **./test/M3FD** folder.
```shell
# Test: use given example and save fused color images to result/M3FD
# If you want to test the custom data, please modify the file path in 'test.py'
python test.py
```
### Train
Before training, you need to download the following pre-trained models:
1. Download [*SAM（VIT-B）*](https://github.com/facebookresearch/segment-anything) pre-trained model and place it in the SAM folder.
2. Download [*Xdecoder(Focal-L last checkpoint)*](https://github.com/microsoft/X-Decoder/tree/main) pre-trained model and place it in the xdecoder folder.
```shell
# Train: Please prepare the custom data and save resluts to result
python train.py
```
### Any Question

If you have any other questions about the code, please open an issue in this repository or email us at  `lhy1415291484@gmail.com`.

If you still have any other questions, please email `rollingplainko@gmail.com`.


