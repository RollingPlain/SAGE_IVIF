# SAGE ğŸ”¥

<div align="center">

[![Paper](https://img.shields.io/badge/Paper-CVPR2025-green)](https://scholar.google.com.hk/scholar?hl=zh-CN&as_sdt=0%2C5&q=Every+SAM+Drop+Counts%3A+Embracing+Semantic+Priors+for+Multi-Modality+Image+Fusion+and+Beyond&btnG=)
[![GitHub Stars](https://img.shields.io/github/stars/RollingPlain/SAGE_IVIF?style=social)](https://github.com/RollingPlain/SAGE_IVIF)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.12+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

Official Code for: **"Every SAM Drop Counts: Embracing Semantic Priors for Multi-Modality Image Fusion and Beyond"**  
*Guanyao Wu, Haoyu Liu, Hongming Fu, Yichuan Peng, Jinyuan Liu, Xin Fan, Risheng Liu*  
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025
- [*[Google Scholar]*](https://scholar.google.com.hk/scholar?hl=zh-CN&as_sdt=0%2C5&q=Every+SAM+Drop+Counts%3A+Embracing+Semantic+Priors+for+Multi-Modality+Image+Fusion+and+Beyond&btnG=)

## ğŸ“ Citation

If our work has been helpful to you, please feel free to cite our paper:

```bibtex
@inproceedings{wu2025sage,
  title={Every SAM Drop Counts: Embracing Semantic Priors for Multi-Modality Image Fusion and Beyond},
  author={Wu, Guanyao and Liu, Haoyu and Fu, Hongming and Peng, Yichuan and Liu, Jinyuan and Fan, Xin and Liu, Risheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}
```

## ğŸŒŸ  Preview of SAGE


![preview](assets/fig1.png)
## ğŸš€  Set Up on Your Own Machine

### Virtual Environment

We strongly recommend that you use Conda as a package manager.

```shell
# create virtual environment
conda create -n sage python=3.10
conda activate sage
# select and install pytorch version yourself (Necessary & Important)
# install requirements package
pip install -r requirements.txt
```
#### Data Preparation

You should put the data in the correct place in the following form.

```
SAGE ROOT
â”œâ”€â”€ data
|   â”œâ”€â”€ test
|   |   â”œâ”€â”€ M3FD
|   |   |   â”œâ”€â”€ Ir # infrared images
|   |   |   â””â”€â”€ Vis # visible images
|   |   â”œâ”€â”€ TNO
|   |   |   â”œâ”€â”€ Ir # infrared images
|   |   |   â””â”€â”€ Vis # visible images
|   |   â”œâ”€â”€ RoadScene
|   |   â””â”€â”€ ...
|   â”œâ”€â”€ train
|   |   â”œâ”€â”€ Ir # infrared images
|   |   â”œâ”€â”€ Vis # visible images
|   |   â”œâ”€â”€ Label # segmentation ground truth masks
|   |   â””â”€â”€ Mask_cache # cached segmentation masks generated by SAM
```
### Test
This code natively supports the same naming for infrared and visible image pairs. An naming example can be found in **./test/M3FD** folder.
```shell
# Test: use given example and save fused color images to result/M3FD
# If you want to test the custom data, please modify the file path in 'test.py'
python test.py
```
### Train
Before training, you need to download the following pre-trained models:
1. Download [*SAMï¼ˆVIT-Bï¼‰*](https://github.com/facebookresearch/segment-anything) pre-trained model and place it in the SAM folder.
2. Download [*Xdecoder(Focal-L last checkpoint)*](https://github.com/microsoft/X-Decoder/tree/main) pre-trained model and place it in the xdecoder folder.
```shell
# Train: Please prepare the custom data and save resluts to result
python train.py
```
### Any Question

If you have any other questions about the code, please open an issue in this repository or email us at  `lhy1415291484@gmail.com`.

If you still have any other questions, please email `rollingplainko@gmail.com`.


